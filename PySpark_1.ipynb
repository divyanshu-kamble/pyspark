{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91c10fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.5\n",
      "  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845499 sha256=a139cf82176117836f89fd14c1e1026f998a1b331afd6b08688eefa9ed1aa6d3\n",
      "  Stored in directory: /Users/dkamble.intern/Library/Caches/pip/wheels/0f/f0/3d/517368b8ce80486e84f89f214e0a022554e4ee64969f46279b\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58babb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f84d3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/02 12:14:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# spark = SparkSession.Builder.appName('Dataframe').getOrCreate()\n",
    "spark=SparkSession.builder.appName('Dataframe').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd31b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cai1184ltnt007.bbrouter:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10a3c4fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "961fff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('Department_Dataset 2.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "686afe44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Dept_name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- travel_required: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9a477f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------------+\n",
      "| ID|Dept_name| location|travel_required|\n",
      "+---+---------+---------+---------------+\n",
      "|  1|       HR|     Pune|            yes|\n",
      "|  2|  Finance|Bangalore|             no|\n",
      "|  3|  Finance|Bangalore|             no|\n",
      "|  4|  Finance|     Pune|             no|\n",
      "|  5|     Tech|   Mumbai|             no|\n",
      "|  6|     Tech|     Pune|             no|\n",
      "|  7|     Tech|Bangalore|            yes|\n",
      "|  8|       HR|Bangalore|             no|\n",
      "|  9|       HR|     Pune|             no|\n",
      "| 10|       HR|     Pune|             no|\n",
      "| 11|       HR|   Mumbai|             no|\n",
      "| 12|       HR|   Mumbai|            yes|\n",
      "| 13|  Finance|Bangalore|            yes|\n",
      "| 14|     Tech|Bangalore|            yes|\n",
      "| 15|     Tech|   Mumbai|            yes|\n",
      "| 16|     Tech|     Pune|            yes|\n",
      "| 17|     Tech|Bangalore|             no|\n",
      "| 18|  Finance|   Mumbai|             no|\n",
      "| 19|       HR|   Mumbai|             no|\n",
      "| 20|  Finance|Bangalore|             no|\n",
      "+---+---------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2cc3930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, Dept_name='HR', location='Pune', travel_required='yes'),\n",
       " Row(ID=2, Dept_name='Finance', location='Bangalore', travel_required='no'),\n",
       " Row(ID=3, Dept_name='Finance', location='Bangalore', travel_required='no'),\n",
       " Row(ID=4, Dept_name='Finance', location='Pune', travel_required='no'),\n",
       " Row(ID=5, Dept_name='Tech', location='Mumbai', travel_required='no')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d88c7598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Dept_name: string, location: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select(['Dept_name','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14555d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+\n",
      "|Dept_name| location|\n",
      "+---------+---------+\n",
      "|       HR|     Pune|\n",
      "|  Finance|Bangalore|\n",
      "|  Finance|Bangalore|\n",
      "|  Finance|     Pune|\n",
      "|     Tech|   Mumbai|\n",
      "|     Tech|     Pune|\n",
      "|     Tech|Bangalore|\n",
      "|       HR|Bangalore|\n",
      "|       HR|     Pune|\n",
      "|       HR|     Pune|\n",
      "|       HR|   Mumbai|\n",
      "|       HR|   Mumbai|\n",
      "|  Finance|Bangalore|\n",
      "|     Tech|Bangalore|\n",
      "|     Tech|   Mumbai|\n",
      "|     Tech|     Pune|\n",
      "|     Tech|Bangalore|\n",
      "|  Finance|   Mumbai|\n",
      "|       HR|   Mumbai|\n",
      "|  Finance|Bangalore|\n",
      "+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Dept_name','location']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9be0006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ID', 'int'),\n",
       " ('Dept_name', 'string'),\n",
       " ('location', 'string'),\n",
       " ('travel_required', 'string')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark.withColumn('travel',)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1b89257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "matches = df_pyspark[\"travel_required\"].isin(\"yes\")\n",
    "new_df = df_pyspark.withColumn(\"travel_required\", when(matches, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ca49af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------+---------------+\n",
      "| ID|Dept_name| location|travel_required|\n",
      "+---+---------+---------+---------------+\n",
      "|  1|       HR|     Pune|              1|\n",
      "|  2|  Finance|Bangalore|              0|\n",
      "|  3|  Finance|Bangalore|              0|\n",
      "|  4|  Finance|     Pune|              0|\n",
      "|  5|     Tech|   Mumbai|              0|\n",
      "|  6|     Tech|     Pune|              0|\n",
      "|  7|     Tech|Bangalore|              1|\n",
      "|  8|       HR|Bangalore|              0|\n",
      "|  9|       HR|     Pune|              0|\n",
      "| 10|       HR|     Pune|              0|\n",
      "| 11|       HR|   Mumbai|              0|\n",
      "| 12|       HR|   Mumbai|              1|\n",
      "| 13|  Finance|Bangalore|              1|\n",
      "| 14|     Tech|Bangalore|              1|\n",
      "| 15|     Tech|   Mumbai|              1|\n",
      "| 16|     Tech|     Pune|              1|\n",
      "| 17|     Tech|Bangalore|              0|\n",
      "| 18|  Finance|   Mumbai|              0|\n",
      "| 19|       HR|   Mumbai|              0|\n",
      "| 20|  Finance|Bangalore|              0|\n",
      "+---+---------+---------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf9a26b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478f001",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19acb002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc93d18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16716b56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
